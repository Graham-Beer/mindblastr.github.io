---
layout: post
title: CCA131 Study and Feedback
subtitle: My experience with the Cloudera Administrator Certification exam, and some studying tips
category: learning
tags: [certifications, cloudera]
image: /images/cloudera.jpg
menubar: true
time_to_read: true
hero_image: /images/study.jpg
---

So, I've attended the exam before they reworked the platform on which they evaulated people, but the requests remain the same. You can find more information from Cloudera on the exam [here](https://www.cloudera.com/about/training/certification/cca-admin.html).

If you want to know more about how and what to study for the exam you can skip to the following sections.

This exam expects you to have previous Linux SysAdmin Knowledge, and some Cloudera Administration knowledge. If you've never worked with Cloudera before, or don't have a modicum of experience using a terminal and solving administrative issues, this is not going to be easy for you :D.

If you're comfortable around the Cloudera Manager, and have debugged problems with JDK versions in machines, you're probably going to do just fine on the exam. It's not terribly complicated, but it is tricky. During the next sections I also speak a bit of my experience throught the preparation for the exam, and the actual exam. 

Note that you really do have to take to the letter the requirements for your desk, the room you're taking the exam at, and noise. I had to switch rooms before I started the exam because "there was too much" on my desk and walls. Also, they don't really tell you this anywhere, but you're not supposed to have headphones on, and they will ask you to take them off. Make sure your cam works, and if you are on a laptop be ready to move your laptop around to show the examinor your room. 

My connection at the time was pretty slow, and it will stress you out if you don't take deep breaths. I think they mostly fixed those delay issues with the new platform, but if they didn't.. Well, be ready. 

Also, you can consult some Cloudera and Apache documentation during the exam, so if you know already some locations of important information, you can search for it. I didn't have a lot of luck with it because it was extremely slow and just hindered my progress, but hopefully wasn't needed in my case, but you should be ready for it failing on you or knowing how to get there if you really need to. 

## Studying for the Exam

In this case, and in almost all the others, Google is your friend :), so searching for feedback on the exam, answers to your questions and diving into the Cloudera and Apache Documentation is something you should be doing often.

On this line, I can tell you that [cca131 Archives \| Hadoop and Cloud](https://www.hadoopandcloud.com/cca131/) has cool and detailed information on each exam requirement.

If you haven't checked it out yet, the [video](https://www.youtube.com/watch?v=CY_XK1kPcTw) that was previously showcased on the Cloudera exam page was pretty interesting and gave neat insights into how the questions were structured. 

Since this is a hands-on exam, you can't get away with knowing how to do things "in theory". The Cloudera Hadoop environment has a lot of ins and outs that you'll only know about by doing. My advice would be for you to make up a Checklist of items that you need to understand better, and learn how to do by heart based on your experience, and the exam requirements, and actually get to it. 

So, if you've never setup a Cloudera Cluster before, this should be a great excuse to start. Setup a few VMs locally or in the cloud or give it a go with containers and get at it! Install a cluster with HDFS, Hive, Hue, Kafka, Kudu, Impala and all their required dependencies. You can also try and set up a local CDH repository and take one item out of the list!

If you've set one up before, or you have a test cluster laying around, get ready to give it some use, potentially break it, and hopefully fix it! If you break the test cluster in a way that most of the services are unusable, say brick the HDFS, you're pretty much done on the exam, so double check the state of your cluster after every change.

### My Knowledge Checklist

So, during my study I composed these items to look at, and learn how to do, based on the exam requirements, and other information I could gather:

- [ ] HDFS 
	- [ ] Setting up High Availabilty
	- [ ] How to make and restore a Checkpoint
	- [ ] How to make and restore Snapshots
	- [ ] How to change block size
	- [ ] What is the Sticky Bit
	- [ ] How to use and change ACLs
	- [ ] The fsck and dfsadmin commands
	- [ ] Load Balancing with a Specific Bandwidht (Balancer role and terminal commands)
	- [ ] How does HttpFs work
	- [ ] Changing and Setting up Trash
- [ ] Cloudera Manager
	- [ ] Set up Alerting
	- [ ] OS Level Configurations required
	- [ ] Make and use Host Templates
	- [ ] Make a CDH Repo
	- [ ] Make and use Role Groups
	- [ ] Set up and use Racks
- [ ] Other Services
	- [ ] Set up a proxy for Hive/Impala
	- [ ] Install and perform a Sqoop from MySQL to Hive
	- [ ] Look at Flume, and try to use it.
	- [ ] Hue/Sentry authorization/authentication
	- [ ] Look at Yarn Fair Scheduler
	- [ ] Change Yarn/Impala pools

You should also look into the several service's settings that you have access through CM, you might be asked to changed some of those settings for a different number of reasons. 

Based on this checklist and my study I also wrote some notes.

## General Information

### Change Block Size
`hdfs dfs -D dfs.block.size=134217728 -cp /origin /dest`

### Sqoop 
- https://dzone.com/articles/sqoop-import-data-from-mysql-to-hive

### Snapshots
- https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html

### Decomission Datanodes
- https://docs.cloudera.com/documentation/enterprise/5-2-x/topics/cm_mc_decomm_host.html#cmug_topic_7_8__section_wq5_ztq_nm

### Compression 
- Too complicated, use documentation

### Disk Alert
- Set up Alert Publisher by enabling email alerts and give the needed information
- On HDFS config enable alerts for this service
- The search for Free Space Threshold and change

### HDFS Balancer
- Balance the Cluster
- To change the balancing threshold CM - HDFS - Balancer - rebalancing threshold (default is 10)
- The property ‘Maximum Concurrent Moves’ sets the maximum number of threads used by the DataNode balancer
- Run the balancer at a specific bandwidht: hdfs dfsadmin -setBalancerBandwidth <bandwidth in bytes per second> 1024^2 bytes = 1mb

### Hive/Impala Proxy
#### HS2
- HiveServers run on default port 10000
- HS2 instances and Haproxy should be on different servers
- yum install haproxy
- vi /etc/haproxy/haproxy.cfg
>listen hiveserver2 :10000
>mode tcp
>option tcplog
>balance leastconn
>server server1 master:10000
>server server2 standby:10000
- Start and enable
> service haproxy start
> chkconfig haproxy on
- On CM Hive Config provide the addres for the Load Balancer
- Test: `beeline –u ‘jdbc:hive2://server3:10000/default’` or through Hue

#### Impala 
- Impala daemon default is 21050
- HAProxy should listen on 21000
- Config:
>listen impala :21000
>mode tcp
>option tcplog
>balance leastconn
>server server1 master:21050
>server server2 standby:21050
>server server3 slave1:21050

- On CM Impala Config provide the address for the Load Balancer
- Test: `jdbc:impala://server3:21000` or through Hue

### OS Level Config
- Disable SELINUX
>vi /etc/sysconfig/selinux
>selinux=disabled
- Set vm.swappiness to 10 or less.
>sysctl vm.swappiness=10
>sysctl -p

- Verify Hostnames, edit /etc/hosts if there is no suitable DNS resolution

- Additional: Set noatime so it doesn't store the blocks accessed time in memory

>cat /etc/fstab
/dev/sda2 /hadoop ext3 noatime,defaults 0 0

- Disable Transparent Huge Pages (THP)

>vi /etc/rc.local
echo 'never' > /sys/kernel/mm/redhat_transparent_hugepage/defrag (also run this)

### CDH Repo
#### RPMs
- Download the repo into the machine: `wget https://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo`
- Move cloudera-cdh5.repo to /etc/yum.repos.d
- Install webserver: `yum install httpd` and start the service
- Install yum-utils and createrepo
- Fetch the repo: `reposync -r cloudera-cdh5`
- Copy the RPM's on to /var/www/html/cdh/5/rpms/ 
- Inside /var/www/html/cdh/5/ run `createrepo`
- Edit the first file downloaded and replace baseurl with http://servername/cdh/5/
- Distribute the file

#### Parcels
- Install webserver: `yum install httpd and start the service`
- `sudo mkdir -p /var/www/html/cloudera-repos`
- `sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh5/parcels/5.14.4/ -P /var/www/html/cloudera-repos`
- `sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/gplextras5/parcels/5.14.4/ -P /var/www/html/cloudera-repos`
- `sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cdh5`
- `sudo chmod -R ugo+rX /var/www/html/cloudera-repos/gplextras5`

### Checkpoints
Where is the VERSION file? - It's located in the fsimage directory in the namenode (or secondary namenode?) directory

- [Backing Up and Restoring NameNode Metadata](https://docs.cloudera.com/documentation/enterprise/5-8-x/topics/cm_mc_nn_metadata_backup.html)
- Force Checkpoint:
>hdfs dfsadmin -safemode enter
hdfs dfsadmin -saveNamespace
hdfs dfsadmin -safemode leave

### HDFS High Availability
Requires more than 1 Journal Node? More than one Failover Controller?
3 Journal Nodes minimum. 
- [Enabling HDFS HA](https://docs.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_hag_hdfs_ha_enabling.html#cmug_topic_5_12)

## References 
- [Walkthrough through all the exam topics](https://www.hadoopandcloud.com/cca131/)
- [HDFS Admin Guide - Horton Works](https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.3.0/bk_hdfs_admin_tools/content/files_and_directories.html)
- [HDFS Command Guide](https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfsadmin)
- [Getting Start With Hadoop Tutorial](https://www.cloudera.com/developers/get-started-with-hadoop-tutorial.html)
- [Preparing for CCA131](https://www.thegeekdiary.com/preparing-for-the-cca-administrator-exam-cca131/)